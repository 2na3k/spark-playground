{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "pyspark.__version__\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import Row, SQLContext\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOS:\n",
    "\n",
    "Actually to keep track with the original usecases:\n",
    "- Due to the need of customization for the name of the output files -> in Glue, using `toPandas()` to generate the `.csv` files.\n",
    "- Enable PyArrow for reduce the memory footprint on df to df conversion.\n",
    "- Reduce the number of workers (5 to 2 DPU) and increases the size of each workers (16 to 32GB) -> Need to calculate the exact distribution.\n",
    "- Is there anyway else to reduce the exchange in the cluster (of fucking course if there is no cluster to be exchanged -> auto on machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes about PyArrow:\n",
    "- Use JVM memories (on or off heap I'm not sure)\n",
    "- Only support for several data types, and if there is any version mismatch -> won't work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "- Run `docker compose up -scale spark-worker=3` to spin up clusters\n",
    "- Install whatever needed from the thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/17 10:53:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://k-comp:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>testing optimization</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7e86cbd8b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"testing optimization\")\n",
    "    # .master(\"spark://localhost:7077\")\n",
    "    # .config(\"spark.sql.execution.arrow.pyspark.selfDestruct.enabled\", \"true\")\n",
    "    # .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actually do the thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>sum_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USA</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UK</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Canada</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country  sum_id\n",
       "0     USA       9\n",
       "1      UK       2\n",
       "2  Canada       4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/17 10:53:32 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(\n",
    "    [\n",
    "        (1, \"John\", \"USA\"),\n",
    "        (2, \"Mary\", \"UK\"),\n",
    "        (3, \"David\", \"USA\"),\n",
    "        (4, \"Emily\", \"Canada\"),\n",
    "        (5, \"Robert\", \"USA\"),\n",
    "    ],\n",
    "    [\"id\", \"name\", \"country\"],\n",
    ")\n",
    "\n",
    "# Create DataFrame 2\n",
    "df2 = spark.createDataFrame(\n",
    "    [\n",
    "        (1, \"New York\", \"USA\"),\n",
    "        (2, \"London\", \"UK\"),\n",
    "        (3, \"Los Angeles\", \"USA\"),\n",
    "        (4, \"Toronto\", \"Canada\"),\n",
    "        (5, \"Chicago\", \"USA\"),\n",
    "    ],\n",
    "    [\"id\", \"city\", \"country\"],\n",
    ")\n",
    "\n",
    "\n",
    "df3 = (\n",
    "    df1.join(F.broadcast(df2), df1.id == df2.id, \"left\")\n",
    "    .drop(df1.id)\n",
    "    .groupBy(df2.country)\n",
    "    .agg(F.sum(\"id\").alias(\"sum_id\"))\n",
    "    .withColumn(\"sum_id\", F.col(\"sum_id\").cast(IntegerType()))\n",
    ")\n",
    "\n",
    "df3.schema\n",
    "pd_df = df3.toPandas()\n",
    "\n",
    "\n",
    "pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [country#8, cast('sum_id as int) AS sum_id#38]\n",
      "+- Aggregate [country#8], [country#8, sum(id#6L) AS sum_id#35L]\n",
      "   +- Project [name#1, country#2, id#6L, city#7, country#8]\n",
      "      +- Join LeftOuter, (id#0L = id#6L)\n",
      "         :- LogicalRDD [id#0L, name#1, country#2], false\n",
      "         +- ResolvedHint (strategy=broadcast)\n",
      "            +- LogicalRDD [id#6L, city#7, country#8], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "country: string, sum_id: int\n",
      "Project [country#8, cast(sum_id#35L as int) AS sum_id#38]\n",
      "+- Aggregate [country#8], [country#8, sum(id#6L) AS sum_id#35L]\n",
      "   +- Project [name#1, country#2, id#6L, city#7, country#8]\n",
      "      +- Join LeftOuter, (id#0L = id#6L)\n",
      "         :- LogicalRDD [id#0L, name#1, country#2], false\n",
      "         +- ResolvedHint (strategy=broadcast)\n",
      "            +- LogicalRDD [id#6L, city#7, country#8], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [country#8], [country#8, cast(sum(id#6L) as int) AS sum_id#38]\n",
      "+- Project [id#6L, country#8]\n",
      "   +- Join LeftOuter, (id#0L = id#6L), rightHint=(strategy=broadcast)\n",
      "      :- Project [id#0L]\n",
      "      :  +- LogicalRDD [id#0L, name#1, country#2], false\n",
      "      +- Project [id#6L, country#8]\n",
      "         +- Filter isnotnull(id#6L)\n",
      "            +- LogicalRDD [id#6L, city#7, country#8], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=true\n",
      "+- == Final Plan ==\n",
      "   *(3) HashAggregate(keys=[country#8], functions=[sum(id#6L)], output=[country#8, sum_id#38])\n",
      "   +- AQEShuffleRead coalesced\n",
      "      +- ShuffleQueryStage 1\n",
      "         +- Exchange hashpartitioning(country#8, 200), ENSURE_REQUIREMENTS, [plan_id=102]\n",
      "            +- *(2) HashAggregate(keys=[country#8], functions=[partial_sum(id#6L)], output=[country#8, sum#42L])\n",
      "               +- *(2) Project [id#6L, country#8]\n",
      "                  +- *(2) BroadcastHashJoin [id#0L], [id#6L], LeftOuter, BuildRight, false\n",
      "                     :- *(2) Project [id#0L]\n",
      "                     :  +- *(2) Scan ExistingRDD[id#0L,name#1,country#2]\n",
      "                     +- BroadcastQueryStage 0\n",
      "                        +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [plan_id=60]\n",
      "                           +- *(1) Project [id#6L, country#8]\n",
      "                              +- *(1) Filter isnotnull(id#6L)\n",
      "                                 +- *(1) Scan ExistingRDD[id#6L,city#7,country#8]\n",
      "+- == Initial Plan ==\n",
      "   HashAggregate(keys=[country#8], functions=[sum(id#6L)], output=[country#8, sum_id#38])\n",
      "   +- Exchange hashpartitioning(country#8, 200), ENSURE_REQUIREMENTS, [plan_id=40]\n",
      "      +- HashAggregate(keys=[country#8], functions=[partial_sum(id#6L)], output=[country#8, sum#42L])\n",
      "         +- Project [id#6L, country#8]\n",
      "            +- BroadcastHashJoin [id#0L], [id#6L], LeftOuter, BuildRight, false\n",
      "               :- Project [id#0L]\n",
      "               :  +- Scan ExistingRDD[id#0L,name#1,country#2]\n",
      "               +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [plan_id=35]\n",
      "                  +- Project [id#6L, country#8]\n",
      "                     +- Filter isnotnull(id#6L)\n",
      "                        +- Scan ExistingRDD[id#6L,city#7,country#8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.explain(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
